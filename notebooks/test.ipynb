{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6bc26f33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36138    avoir propos jeu nkpro racing disposer moteur ...\n",
      "68630    exit être jeu reprendre sensation escape game ...\n",
      "36172    bricolage lettre mousse argent alphabet autoco...\n",
      "9830     nostalgique fer blanc clockwork chaine jouet p...\n",
      "28422                          courrier unesco mai silence\n",
      "Name: description_propre, dtype: object\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Script de prétraitement de données textuelles pour la classification de produits Rakuten.\n",
    "\n",
    "Ce script effectue les étapes suivantes :\n",
    "1. Chargement des fichiers X_train et Y_train\n",
    "2. Nettoyage des descriptions produits :\n",
    "   - Minuscule\n",
    "   - Suppression des accents\n",
    "   - Nettoyage HTML et ponctuation\n",
    "   - Suppression des stopwords personnalisés\n",
    "   - Lemmatisation en français avec spaCy\n",
    "3. Vectorisation TF-IDF des textes nettoyés\n",
    "4. Affichage d’un aperçu de la matrice TF-IDF pour vérification\n",
    "\n",
    "Classes et fonctions clés :\n",
    "- NettoyeurTexte : transformateur personnalisé scikit-learn pour nettoyage + lemmatisation\n",
    "- remove_accents : fonction utilitaire pour supprimer les accents\n",
    "- lemmatize : fonction pour extraire les lemmes pertinents avec spaCy\n",
    "\n",
    "Utilisation :\n",
    "- Ce script est conçu pour être intégré dans un pipeline scikit-learn complet.\n",
    "- Il peut être facilement connecté à un classifieur (ex: LogisticRegression) pour faire de la prédiction de prdtypecode.\n",
    "\n",
    "Pré-requis :\n",
    "- spaCy installé avec le modèle français : `python -m spacy download fr_core_news_sm`\n",
    "- Fichiers CSV : X_train_update.csv et Y_train_CVw08PX.csv dans le dossier `../data`\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "import unicodedata\n",
    "import spacy\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import string\n",
    "from collections import Counter\n",
    "\n",
    "X_train = pd.read_csv('../data/X_train_update.csv')\n",
    "y_train = pd.read_csv('../data/Y_train_CVw08PX.csv')\n",
    "\n",
    "df = X_train.copy()\n",
    "df['prdtypecode'] = y_train['prdtypecode']\n",
    "nlp = spacy.load(\"fr_core_news_sm\")\n",
    "\n",
    "stopwords_fr = set([\n",
    "    \"le\", \"la\", \"les\", \"un\", \"une\", \"des\", \"de\", \"du\", \"au\", \"aux\", \"en\",\n",
    "    \"et\", \"à\", \"pour\", \"par\", \"avec\", \"sur\", \"dans\", \"ce\", \"ces\", \"se\", \"sa\",\n",
    "    \"son\", \"ses\", \"qui\", \"que\", \"quoi\", \"dont\", \"où\", \"comme\", \"est\", \"sont\",\n",
    "    \"il\", \"elle\", \"ils\", \"elles\", \"nous\", \"vous\", \"ne\", \"pas\", \"plus\", \"moins\",\n",
    "    \"ou\", \"mais\", \"donc\", \"or\", \"ni\", \"car\"\n",
    "])\n",
    "\n",
    "def remove_accents(text):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFKD', text)\n",
    "        if not unicodedata.combining(c)\n",
    "    )\n",
    "\n",
    "def lemmatize(text):\n",
    "    doc = nlp(text)\n",
    "    return \" \".join([token.lemma_ for token in doc if token.lemma_ not in stopwords_fr and len(token.lemma_) > 2 and not token.is_punct and not token.is_space])\n",
    "\n",
    "class NettoyeurTexte(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Nettoyeur de texte personnalisé pour scikit-learn.\n",
    "\n",
    "    Applique un nettoyage linguistique de base :\n",
    "    - Passage en minuscules\n",
    "    - Suppression des accents\n",
    "    - Suppression des balises HTML, ponctuation et chiffres\n",
    "    - Lemmatisation avec spaCy (fr_core_news_sm)\n",
    "    - Suppression des stopwords et des tokens courts\n",
    "    \"\"\"\n",
    "\n",
    "    def fit(self, X: pd.DataFrame, y=None) -> \"NettoyeurTexte\":\n",
    "        \"\"\"\n",
    "        Ne fait rien. Nécessaire pour compatibilité avec l'API scikit-learn.\n",
    "\n",
    "        Args:\n",
    "            X (pd.DataFrame): DataFrame avec au moins les colonnes 'description' et 'designation'.\n",
    "            y (Any, optional): Données cibles (non utilisées ici). Defaults to None.\n",
    "\n",
    "        Returns:\n",
    "            NettoyeurTexte: L'instance de ce transformateur (self).\n",
    "        \"\"\"\n",
    "        return self\n",
    "\n",
    "    def transform(self, X: pd.DataFrame, y=None) -> list[str]:\n",
    "        \"\"\"\n",
    "        Applique le nettoyage et la lemmatisation à chaque ligne du DataFrame.\n",
    "\n",
    "        Args:\n",
    "            X (pd.DataFrame): DataFrame contenant les colonnes 'description' et 'designation'.\n",
    "            y (Any, optional): Ignoré.\n",
    "\n",
    "        Returns:\n",
    "            list[str]: Liste de chaînes nettoyées et lemmatisées.\n",
    "        \"\"\"\n",
    "        texts = []\n",
    "        for _, row in X.iterrows():\n",
    "            text = row['description']\n",
    "            if pd.isnull(text) or str(text).strip() == \"\":\n",
    "                text = row['designation']\n",
    "            if pd.isnull(text):\n",
    "                texts.append(\"\")\n",
    "                continue\n",
    "\n",
    "            text = text.lower()\n",
    "            text = remove_accents(text)\n",
    "            text = re.sub(r'<[^>]+>', ' ', text)\n",
    "            text = re.sub(r'[^a-z\\s]', ' ', text)\n",
    "            text = re.sub(r'\\s+', ' ', text).strip()\n",
    "            texts.append(text)\n",
    "\n",
    "        docs = nlp.pipe(texts, disable=[\"parser\", \"ner\"])\n",
    "        cleaned = []\n",
    "        for doc in docs:\n",
    "            lemmas = [\n",
    "                token.lemma_\n",
    "                for token in doc\n",
    "                if token.lemma_ not in stopwords_fr\n",
    "                and len(token.lemma_) > 2\n",
    "                and not token.is_punct\n",
    "                and not token.is_space\n",
    "            ]\n",
    "            cleaned.append(\" \".join(lemmas))\n",
    "\n",
    "        return cleaned\n",
    "    \n",
    "df_sample = df[['description', 'designation']].sample(5, random_state=42)\n",
    "nettoyeur = NettoyeurTexte()\n",
    "df_sample['description_propre'] = nettoyeur.transform(df_sample)\n",
    "print(df_sample['description_propre'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "62493de5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF shape : (84916, 1000)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Pipeline de traitement texte : \n",
    "\n",
    "    Nettoyage des colonnes 'description' et 'designation' avec le transformateur personnalisé NettoyeurTexte.\n",
    "    → Convertit les textes en minuscules, enlève accents, ponctuation, stopwords, et applique la lemmatisation.\n",
    "\n",
    "    Vectorisation TF-IDF avec TfidfVectorizer :\n",
    "    → Utilise unigrams uniquement, limite à 1000 mots les plus fréquents (max_features)\n",
    "    → Applique un filtrage simple des stopwords anglais    \n",
    "\"\"\"\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('nettoyage', NettoyeurTexte()),\n",
    "    ('tfidf', TfidfVectorizer(\n",
    "        max_features=1000,\n",
    "        ngram_range=(1, 1),  \n",
    "        stop_words='english'  \n",
    "    ))\n",
    "])\n",
    "\n",
    "X_tfidf = pipeline.fit_transform(df[['description', 'designation']])\n",
    "\n",
    "print(\"TF-IDF shape :\", X_tfidf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5044188e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exemple fictif de matrice TF-IDF :\n",
      "      adapt   adhesif   arriere       art     aussi     avoir      bleu  \\\n",
      "0  0.000000  0.000000  0.000000  0.000000  0.000000  0.415983  0.000000   \n",
      "1  0.000000  0.000000  0.000000  0.820413  0.000000  0.095657  0.000000   \n",
      "2  0.169357  0.179139  0.146583  0.000000  0.129054  0.154866  0.118175   \n",
      "3  0.000000  0.000000  0.000000  0.000000  0.000000  0.290655  0.000000   \n",
      "4  0.000000  0.000000  0.000000  0.000000  0.000000  0.106308  0.000000   \n",
      "\n",
      "   caracteristique   confort  couleur  ...     style   support   tactile  \\\n",
      "0          0.00000  0.000000  0.00000  ...  0.000000  0.000000  0.000000   \n",
      "1          0.00000  0.000000  0.00000  ...  0.000000  0.000000  0.000000   \n",
      "2          0.09351  0.142083  0.07601  ...  0.238061  0.281544  0.160019   \n",
      "3          0.00000  0.000000  0.00000  ...  0.000000  0.000000  0.000000   \n",
      "4          0.00000  0.000000  0.00000  ...  0.000000  0.000000  0.000000   \n",
      "\n",
      "    tenir     tout       tre     vente     votre   vouloir      être  \n",
      "0  0.0000  0.00000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
      "1  0.0000  0.00000  0.000000  0.297858  0.000000  0.000000  0.000000  \n",
      "2  0.1522  0.00000  0.107367  0.000000  0.076152  0.000000  0.195175  \n",
      "3  0.0000  0.00000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
      "4  0.0000  0.15919  0.221107  0.000000  0.000000  0.294549  0.133978  \n",
      "\n",
      "[5 rows x 52 columns]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Ce bloc extrait et affiche un exemple de la matrice TF-IDF après transformation.\n",
    "\n",
    "Étapes :\n",
    "    Récupère le TfidfVectorizer utilisé dans le pipeline via named_steps.\n",
    "    Extrait les noms des features (mots/ngrammes) générés par le vectoriseur.\n",
    "    Convertit les 5 premières lignes de la matrice sparse TF-IDF en tableau dense.\n",
    "    Construit un DataFrame pandas lisible avec les colonnes nommées selon les mots.\n",
    "    Supprime les colonnes où tous les scores TF-IDF sont nuls pour simplifier l'affichage.\n",
    "    Affiche les lignes non nulles pour visualiser concrètement l'encodage texte → vecteurs.\n",
    "\n",
    "Utile pour débug ou comprendre la représentation vectorielle des textes.\n",
    "\"\"\"\n",
    "tfidf_vectorizer = pipeline.named_steps['tfidf']\n",
    "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "sample_tfidf = X_tfidf[:5].toarray()\n",
    "df_tfidf_sample = pd.DataFrame(sample_tfidf, columns=feature_names)\n",
    "df_tfidf_sample = df_tfidf_sample.loc[:, (df_tfidf_sample != 0).any(axis=0)]\n",
    "print(\"Exemple fictif de matrice TF-IDF :\")\n",
    "print(df_tfidf_sample.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
