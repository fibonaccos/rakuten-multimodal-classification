{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6bc26f33",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Peeta\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Script de prétraitement de données textuelles pour la classification de produits Rakuten.\n",
    "\n",
    "Ce script effectue les étapes suivantes :\n",
    "1. Chargement des fichiers X_train et Y_train\n",
    "2. Nettoyage des descriptions produits :\n",
    "   - Minuscule\n",
    "   - Suppression des accents\n",
    "   - Nettoyage HTML et ponctuation\n",
    "   - Suppression des stopwords personnalisés\n",
    "   - Lemmatisation en français avec spaCy\n",
    "3. Vectorisation TF-IDF des textes nettoyés\n",
    "4. Affichage d’un aperçu de la matrice TF-IDF pour vérification\n",
    "\n",
    "Classes et fonctions clés :\n",
    "- NettoyeurTexte : transformateur personnalisé scikit-learn pour nettoyage + lemmatisation\n",
    "- remove_accents : fonction utilitaire pour supprimer les accents\n",
    "- lemmatize : fonction pour extraire les lemmes pertinents avec spaCy\n",
    "\n",
    "Utilisation :\n",
    "- Ce script est conçu pour être intégré dans un pipeline scikit-learn complet.\n",
    "- Il peut être facilement connecté à un classifieur (ex: LogisticRegression) pour faire de la prédiction de prdtypecode.\n",
    "\n",
    "Pré-requis :\n",
    "- spaCy installé avec le modèle français : `python -m spacy download fr_core_news_sm`\n",
    "- Fichiers CSV : X_train_update.csv et Y_train_CVw08PX.csv dans le dossier `../data`\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "import unicodedata\n",
    "import spacy\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import string\n",
    "from collections import Counter\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from langdetect import detect\n",
    "from langdetect.lang_detect_exception import LangDetectException\n",
    "\n",
    "X_train = pd.read_csv('../data/X_train_update.csv')\n",
    "y_train = pd.read_csv('../data/Y_train_CVw08PX.csv')\n",
    "\n",
    "df = X_train.copy()\n",
    "df['prdtypecode'] = y_train['prdtypecode']\n",
    "nlp = spacy.load(\"fr_core_news_sm\")\n",
    "\n",
    "STOPWORD = set([\n",
    "    \"le\", \"la\", \"les\", \"un\", \"une\", \"des\", \"de\", \"du\", \"au\", \"aux\", \"en\",\n",
    "    \"et\", \"à\", \"pour\", \"par\", \"avec\", \"sur\", \"dans\", \"ce\", \"ces\", \"se\", \"sa\",\n",
    "    \"son\", \"ses\", \"qui\", \"que\", \"quoi\", \"dont\", \"où\", \"comme\", \"est\", \"sont\",\n",
    "    \"il\", \"elle\", \"ils\", \"elles\", \"nous\", \"vous\", \"ne\", \"pas\", \"plus\", \"moins\",\n",
    "    \"ou\", \"mais\", \"donc\", \"or\", \"ni\", \"car\"\n",
    "])\n",
    "\n",
    "def remove_accents(text):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFKD', text)\n",
    "        if not unicodedata.combining(c)\n",
    "    )\n",
    "\n",
    "def lemmatize(text):\n",
    "    doc = nlp(text)\n",
    "    return \" \".join([token.lemma_ for token in doc if token.lemma_ not in STOPWORD and len(token.lemma_) > 2 and not token.is_punct and not token.is_space])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b4e71bd",
   "metadata": {},
   "source": [
    "PARTIE TRADUCTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6ead98dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "LANGUES = {\n",
    "    \"en\": \"Helsinki-NLP/opus-mt-en-fr\",\n",
    "    \"it\": \"Helsinki-NLP/opus-mt-it-fr\",\n",
    "    \"es\": \"Helsinki-NLP/opus-mt-es-fr\",\n",
    "    \"nl\": \"Helsinki-NLP/opus-mt-nl-fr\",\n",
    "    \"ro\": \"Helsinki-NLP/opus-mt-ro-fr\"\n",
    "}\n",
    "\n",
    "MODELE_TRADUCTION = {}\n",
    "\n",
    "def detection_langue(text):\n",
    "    try:\n",
    "        return detect(text)\n",
    "    except LangDetectException:\n",
    "        return \"inconnu\"\n",
    "    \n",
    "def traduction(text, lang_code):\n",
    "    if lang_code not in LANGUES:\n",
    "        return text\n",
    "    if lang_code not in MODELE_TRADUCTION:\n",
    "        model_name = LANGUES[lang_code]\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "        MODELE_TRADUCTION[lang_code] = (tokenizer, model)\n",
    "    else:\n",
    "        tokenizer, model = MODELE_TRADUCTION[lang_code]\n",
    "\n",
    "    inputs = tokenizer.encode(text, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "    outputs = model.generate(inputs, max_length=512, num_beams=4, early_stopping=True)\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ed753641",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36138    avoir propos jeu nkpro racing disposer moteur ...\n",
      "68630    exit être jeu reprendre sensation escape game ...\n",
      "36172    bricolage lettre mousse argent alphabet autoco...\n",
      "9830     nostalgique fer blanc clockwork chaine jouet p...\n",
      "28422                          courrier unesco mai silence\n",
      "Name: description_propre, dtype: object\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Peeta\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Peeta\\.cache\\huggingface\\hub\\models--Helsinki-NLP--opus-mt-it-fr. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "c:\\Users\\Peeta\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\models\\marian\\tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n",
      "c:\\Users\\Peeta\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Peeta\\.cache\\huggingface\\hub\\models--Helsinki-NLP--opus-mt-ro-fr. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "c:\\Users\\Peeta\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Peeta\\.cache\\huggingface\\hub\\models--Helsinki-NLP--opus-mt-en-fr. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "c:\\Users\\Peeta\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Peeta\\.cache\\huggingface\\hub\\models--Helsinki-NLP--opus-mt-es-fr. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "c:\\Users\\Peeta\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Peeta\\.cache\\huggingface\\hub\\models--Helsinki-NLP--opus-mt-nl-fr. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Données sauvegardées dans X_train_cleaned.csv avec les descriptions nettoyées.\n"
     ]
    }
   ],
   "source": [
    "class NettoyeurTexte(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Nettoyeur de texte personnalisé pour scikit-learn.\n",
    "\n",
    "    Applique un nettoyage linguistique de base :\n",
    "    - Passage en minuscules\n",
    "    - Suppression des accents\n",
    "    - Suppression des balises HTML, ponctuation et chiffres\n",
    "    - Lemmatisation avec spaCy (fr_core_news_sm)\n",
    "    - Suppression des stopwords et des tokens courts\n",
    "    \"\"\"\n",
    "\n",
    "    def fit(self, X: pd.DataFrame, y=None) -> \"NettoyeurTexte\":\n",
    "        \"\"\"\n",
    "        Ne fait rien. Nécessaire pour compatibilité avec l'API scikit-learn.\n",
    "\n",
    "        Args:\n",
    "            X (pd.DataFrame): DataFrame avec au moins les colonnes 'description' et 'designation'.\n",
    "            y (Any, optional): Données cibles (non utilisées ici). Defaults to None.\n",
    "\n",
    "        Returns:\n",
    "            NettoyeurTexte: L'instance de ce transformateur (self).\n",
    "        \"\"\"\n",
    "        return self\n",
    "\n",
    "    def transform(self, X: pd.DataFrame, y=None) -> list[str]:\n",
    "        \"\"\"\n",
    "            Applique le nettoyage et la lemmatisation à chaque ligne du DataFrame.\n",
    "\n",
    "            Args:\n",
    "                X (pd.DataFrame): DataFrame contenant les colonnes 'description' et 'designation'.\n",
    "                y (Any, optional): Ignoré.\n",
    "\n",
    "            Returns:\n",
    "                list[str]: Liste de chaînes nettoyées et lemmatisées.\n",
    "        \"\"\"\n",
    "        texts = []\n",
    "        for _, row in X.iterrows():\n",
    "            text = row['description']\n",
    "            if pd.isnull(text) or str(text).strip() == \"\":\n",
    "                text = row['designation']\n",
    "            if pd.isnull(text):\n",
    "                texts.append(\"\")\n",
    "                continue\n",
    "\n",
    "            text = text.lower()\n",
    "            text = remove_accents(text)\n",
    "\n",
    "            lang = detection_langue(text)\n",
    "            if lang in LANGUES:\n",
    "                text = traduction(text, lang)\n",
    "\n",
    "            text = re.sub(r'<[^>]+>', ' ', text)\n",
    "            text = re.sub(r'\\b\\w*\\d\\w*\\b', ' ', text)\n",
    "            text = re.sub(r'[^a-z\\s]', ' ', text)\n",
    "            text = re.sub(r'\\s+', ' ', text).strip()\n",
    "            texts.append(text)\n",
    "\n",
    "        docs = nlp.pipe(texts, disable=[\"parser\", \"ner\"])\n",
    "        cleaned = []\n",
    "        for doc in docs:\n",
    "            lemmas = [\n",
    "                token.lemma_\n",
    "                for token in doc\n",
    "                if token.lemma_ not in STOPWORD\n",
    "                and len(token.lemma_) > 2\n",
    "                and not token.is_punct\n",
    "                and not token.is_space\n",
    "            ]\n",
    "            cleaned.append(\" \".join(lemmas))\n",
    "\n",
    "        return cleaned\n",
    "\n",
    "df_sample = df[['description', 'designation']].sample(5, random_state=42)\n",
    "nettoyeur = NettoyeurTexte()\n",
    "df_sample['description_propre'] = nettoyeur.transform(df_sample)\n",
    "print(df_sample['description_propre'])\n",
    "\n",
    "df_cleaned = df.copy()\n",
    "df_cleaned['description'] = nettoyeur.transform(df_cleaned[['description', 'designation']])\n",
    "\n",
    "df_cleaned.to_csv(\"X_train_cleaned2.csv\", index=False)\n",
    "print(\" Données sauvegardées dans X_train_cleaned.csv avec les descriptions nettoyées.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62493de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Pipeline de traitement texte : \n",
    "\n",
    "    Nettoyage des colonnes 'description' et 'designation' avec le transformateur personnalisé NettoyeurTexte.\n",
    "    → Convertit les textes en minuscules, enlève accents, ponctuation, stopwords, et applique la lemmatisation.\n",
    "\n",
    "    Vectorisation TF-IDF avec TfidfVectorizer :\n",
    "    → Utilise unigrams uniquement, limite à 1000 mots les plus fréquents (max_features)\n",
    "    → Applique un filtrage simple des stopwords anglais    \n",
    "\"\"\"\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('nettoyage', NettoyeurTexte()),\n",
    "    ('tfidf', TfidfVectorizer(\n",
    "        max_features=1000,\n",
    "        ngram_range=(1, 1),  \n",
    "        stop_words='english'  \n",
    "    ))\n",
    "])\n",
    "\n",
    "X_tfidf = pipeline.fit_transform(df[['description', 'designation']])\n",
    "\n",
    "print(\"TF-IDF shape :\", X_tfidf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5044188e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exemple fictif de matrice TF-IDF :\n",
      "      adapt   adhesif   arriere       art     aussi     avoir      bleu  \\\n",
      "0  0.000000  0.000000  0.000000  0.000000  0.000000  0.415983  0.000000   \n",
      "1  0.000000  0.000000  0.000000  0.820413  0.000000  0.095657  0.000000   \n",
      "2  0.169357  0.179139  0.146583  0.000000  0.129054  0.154866  0.118175   \n",
      "3  0.000000  0.000000  0.000000  0.000000  0.000000  0.290655  0.000000   \n",
      "4  0.000000  0.000000  0.000000  0.000000  0.000000  0.106308  0.000000   \n",
      "\n",
      "   caracteristique   confort  couleur  ...     style   support   tactile  \\\n",
      "0          0.00000  0.000000  0.00000  ...  0.000000  0.000000  0.000000   \n",
      "1          0.00000  0.000000  0.00000  ...  0.000000  0.000000  0.000000   \n",
      "2          0.09351  0.142083  0.07601  ...  0.238061  0.281544  0.160019   \n",
      "3          0.00000  0.000000  0.00000  ...  0.000000  0.000000  0.000000   \n",
      "4          0.00000  0.000000  0.00000  ...  0.000000  0.000000  0.000000   \n",
      "\n",
      "    tenir     tout       tre     vente     votre   vouloir      être  \n",
      "0  0.0000  0.00000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
      "1  0.0000  0.00000  0.000000  0.297858  0.000000  0.000000  0.000000  \n",
      "2  0.1522  0.00000  0.107367  0.000000  0.076152  0.000000  0.195175  \n",
      "3  0.0000  0.00000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
      "4  0.0000  0.15919  0.221107  0.000000  0.000000  0.294549  0.133978  \n",
      "\n",
      "[5 rows x 52 columns]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Ce bloc extrait et affiche un exemple de la matrice TF-IDF après transformation.\n",
    "\n",
    "Étapes :\n",
    "    Récupère le TfidfVectorizer utilisé dans le pipeline via named_steps.\n",
    "    Extrait les noms des features (mots/ngrammes) générés par le vectoriseur.\n",
    "    Convertit les 5 premières lignes de la matrice sparse TF-IDF en tableau dense.\n",
    "    Construit un DataFrame pandas lisible avec les colonnes nommées selon les mots.\n",
    "    Supprime les colonnes où tous les scores TF-IDF sont nuls pour simplifier l'affichage.\n",
    "    Affiche les lignes non nulles pour visualiser concrètement l'encodage texte → vecteurs.\n",
    "\n",
    "Utile pour débug ou comprendre la représentation vectorielle des textes.\n",
    "\"\"\"\n",
    "tfidf_vectorizer = pipeline.named_steps['tfidf']\n",
    "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "sample_tfidf = X_tfidf[:5].toarray()\n",
    "df_tfidf_sample = pd.DataFrame(sample_tfidf, columns=feature_names)\n",
    "df_tfidf_sample = df_tfidf_sample.loc[:, (df_tfidf_sample != 0).any(axis=0)]\n",
    "print(\"Exemple fictif de matrice TF-IDF :\")\n",
    "print(df_tfidf_sample.head())\n",
    "\n",
    "feature_names = pipeline.get_feature_names_out()\n",
    "df_tfidf = pd.DataFrame(X_tfidf.toarray(), columns=feature_names)\n",
    "df_tfidf.insert(0, \"productid\", df['productid'].values)\n",
    "df_tfidf.to_csv(\"tfidf_vectorized_matrix.csv\", index=False)\n",
    "print(\"Matrice TF-IDF enregistrée dans : tfidf_vectorized_matrix.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
