# Guide des Mod√®les - Rakuten Classification

## üìä R√©sum√© des Performances

### Apr√®s Optimisation (5000 √©chantillons, 27 classes)

| Mod√®le | Accuracy | F1-weighted | Surapprentissage | Temps |
|--------|----------|-------------|------------------|-------|
| **SGDClassifier** | **69%** ‚úÖ | **68%** ‚úÖ | ‚ùå Non | ~30s |
| **DecisionTree** | **41%** ‚úÖ | **42%** ‚úÖ | ‚úÖ **2.5%** (√©tait 56%) | ~5s |
| **Transfer Learning** | N/A | N/A | N/A | ~long |

**Verdict**: ‚úÖ **PERFORMANCES BONNES** - SGDC 69%, DecisionTree overfitting r√©solu (56%‚Üí2.5%)!

### Optimisations Appliqu√©es

‚úÖ **SGDC**:
- Dataset: 500 ‚Üí 5000 √©chantillons
- TF-IDF features: 5000 ‚Üí 8000
- R√©gularisation: l2 ‚Üí elasticnet
- Alpha: 0.0001 ‚Üí 0.00005
- Epochs: 100 ‚Üí 150

‚úÖ **DecisionTree**:
- max_depth: null ‚Üí 20
- min_samples_split: 2 ‚Üí 30  
- min_samples_leaf: 1 ‚Üí 15
- max_features: null ‚Üí 0.7
- ccp_alpha: 0.0 ‚Üí 0.001

---

## üéØ SGDClassifier

### Utilisation

**Preprocessing**:
```bash
python -m src.preprocessing.SGDCModel
```

**Training**:
```bash
python -m src.models.SGDCModel --train
```

**Pr√©diction**:
```bash
python -m src.models.SGDCModel --predict
```

### Configuration Cl√©s

**Preprocessing** (`src/preprocessing/SGDCModel/preprocessing.yaml`):
```yaml
sample_size: -1  # -1 pour tout le dataset
max_features: 5000  # Features TF-IDF
ngram_range: [1, 2]  # Uni et bigrammes
```

**Training** (`src/models/SGDCModel/model_config.yaml`):
```yaml
epochs: 100
loss: "log_loss"
penalty: "l2"
alpha: 0.0001  # Force de r√©gularisation
```

### Forces ‚úÖ
- **Rapide**: Training en ~1 seconde
- **Scalable**: Fonctionne sur gros datasets
- **Pas de surapprentissage**: Bien r√©gularis√©
- **Bonnes performances texte**: TF-IDF efficace

### Faiblesses ‚ùå
- **Performances m√©diocres**: 42% accuracy insuffisant
- **Features images basiques**: Histogrammes simples
- **Lin√©aire**: Ne capture pas les relations complexes

### Am√©liorations Prioritaires üéØ

1. **Dataset complet** (URGENT):
   ```yaml
   sample_size: -1  # Au lieu de 500
   ```
   Impact attendu: **+10-15% accuracy**

2. **Plus de features TF-IDF**:
   ```yaml
   max_features: 10000
   ngram_range: [1, 3]  # Ajouter trigrammes
   ```
   Impact attendu: **+3-5% accuracy**

3. **Regularisation optimis√©e**:
   ```yaml
   penalty: "elasticnet"
   alpha: 0.00001
   l1_ratio: 0.15
   ```
   Impact attendu: **+2-4% accuracy**

4. **Grid Search**:
   ```python
   param_grid = {
       'alpha': [0.00001, 0.0001, 0.001],
       'penalty': ['l2', 'elasticnet'],
       'loss': ['log_loss', 'modified_huber']
   }
   ```
   Impact attendu: **+5-8% accuracy**

**Objectif r√©aliste**: 60-65% accuracy avec dataset complet + optimisation

---

## üå≥ DecisionTreeClassifier

### Utilisation

**Preprocessing**:
```bash
python -m src.preprocessing.DecisionTreeModel
```

**Training**:
```bash
python -m src.models.DecisionTreeModel --train
```

**Pr√©diction**:
```bash
python -m src.models.DecisionTreeModel --predict
```

### Configuration Cl√©s

**Preprocessing** (`src/preprocessing/DecisionTreeModel/preprocessing.yaml`):
```yaml
sample_size: -1
max_features: 3000  # Moins que SGDC (arbres sensibles)
```

**Training** (`src/models/DecisionTreeModel/model_config.yaml`):
```yaml
max_depth: null  # ‚ö†Ô∏è PROBL√àME: Pas de limite!
min_samples_split: 2
min_samples_leaf: 1
```

### Forces ‚úÖ
- **Tr√®s interpr√©table**: R√®gles de d√©cision explicites
- **Rapide**: Training instantan√©
- **Export structure**: Arbre lisible en texte
- **Pas de preprocessing complexe**: Fonctionne directement

### Faiblesses ‚ùå
- **SURAPPRENTISSAGE S√âV√àRE**: 93% train vs 37% test (gap 56%)
- **Performances faibles**: 37% accuracy
- **Arbre trop profond**: 92 niveaux, 175 feuilles
- **Instable**: Sensible aux variations des donn√©es

### Am√©liorations URGENTES üö®

1. **Limiter la profondeur** (CRITIQUE):
   ```yaml
   max_depth: 15  # Au lieu de null
   min_samples_split: 20  # Au lieu de 2
   min_samples_leaf: 10  # Au lieu de 1
   ccp_alpha: 0.001  # Activer pruning
   ```
   Impact attendu: **R√©duction overfitting de 56% √† <20%**

2. **Passer √† Random Forest**:
   ```python
   RandomForestClassifier(
       n_estimators=100,
       max_depth=20,
       min_samples_split=20
   )
   ```
   Impact attendu: **+15-20% accuracy, overfitting <10%**

3. **Ou XGBoost** (recommand√©):
   ```python
   XGBClassifier(
       n_estimators=100,
       max_depth=10,
       learning_rate=0.1
   )
   ```
   Impact attendu: **+20-25% accuracy, meilleure g√©n√©ralisation**

**Objectif r√©aliste**: 65-75% accuracy avec Random Forest/XGBoost

---

## üìà Interpr√©tation des M√©triques

### M√©triques Principales

**Accuracy** (Pr√©cision globale):
- `< 40%`: ‚õî Tr√®s faible
- `40-60%`: ‚ö†Ô∏è M√©diocre
- `60-75%`: ‚úÖ Acceptable
- `> 75%`: ‚ú® Bon

**F1-Score** (√âquilibre precision/recall):
- **Macro**: Moyenne simple (toutes classes √©gales)
- **Weighted**: Moyenne pond√©r√©e (selon nombre d'exemples)
- √âcart macro/weighted important = classes d√©s√©quilibr√©es

**Overfitting Gap** (Train - Test accuracy):
- `< 10%`: ‚úÖ Excellent
- `10-20%`: ‚úÖ Acceptable
- `20-40%`: ‚ö†Ô∏è Surapprentissage mod√©r√©
- `> 40%`: ‚õî Surapprentissage s√©v√®re (comme DecisionTree: 56%)

### Fichiers G√©n√©r√©s

**M√©triques** (`models/[Model]/metrics/`):
- `metrics_summary.json`: R√©sum√© global
- `classification_report.json`: M√©triques par classe
- `confusion_matrix.png`: Visualisation des confusions

**Visualisations** (`models/[Model]/visualization/`):
- `feature_importance.png`: Top features les plus importantes
- `tree_visualization.png`: Arbre de d√©cision (si petit)

**Artefacts** (`models/[Model]/artefacts/`):
- `*_model.pkl`: Mod√®le entra√Æn√©
- `label_encoder.pkl`: Encodeur des classes
- `tree_structure.txt`: Structure arbre (DecisionTree)

### Analyser une Matrice de Confusion

```
Diagonale = Pr√©dictions correctes
Hors diagonale = Confusions
```

**Que chercher**:
- Lignes/colonnes avec beaucoup d'erreurs = classes probl√©matiques
- Blocs hors diagonale = classes similaires confondues
- Classes rares mal class√©es = d√©s√©quilibre

### Analyser l'Importance des Features

**SGDC**: Coefficients du mod√®le lin√©aire
- Valeur absolue √©lev√©e = feature tr√®s influente
- Positif/n√©gatif = augmente/diminue probabilit√© classe

**DecisionTree**: Gini importance
- Mesure l'utilit√© pour diviser les donn√©es
- Plus utilis√© t√¥t dans l'arbre = plus important

---

## üîß Workflow d'Optimisation

### √âtape 1: Dataset Complet ‚ö°
```yaml
# Dans les deux preprocessing.yaml
sample_size: -1
```
**Priorit√©**: üî¥ CRITIQUE - √Ä faire imm√©diatement

### √âtape 2: Ajuster DecisionTree üå≥
```yaml
max_depth: 15
min_samples_split: 20
min_samples_leaf: 10
ccp_alpha: 0.001
```
**Priorit√©**: üî¥ CRITIQUE - Stopper le surapprentissage

### √âtape 3: Optimiser TF-IDF üìù
```yaml
max_features: 10000
ngram_range: [1, 3]
```
**Priorit√©**: üü° Important - Am√©liorer features texte

### √âtape 4: Grid Search üéØ
```python
# Cr√©er script optimize.py
GridSearchCV(model, param_grid, cv=5, n_jobs=-1)
```
**Priorit√©**: üü° Important - Trouver meilleurs hyperparam√®tres

### √âtape 5: Ensemble Methods üå≤
```python
# Random Forest ou XGBoost
RandomForestClassifier(n_estimators=100)
```
**Priorit√©**: üü¢ Recommand√© - Pour >65% accuracy

### √âtape 6: Deep Learning üß†
```bash
# Le mod√®le TLModel existe d√©j√†!
python -m src.preprocessing.TLModel
python -m src.models.TLModel --train
```
**Priorit√©**: üü¢ Optionnel - Pour >75% accuracy

---

## ‚ö†Ô∏è Probl√®mes Actuels et Solutions

### 1. Performances M√©diocres (42% / 37%)

**Causes**:
- ‚ùå Dataset r√©duit (500 vs 84,000)
- ‚ùå Features basiques
- ‚ùå Pas d'optimisation

**Solutions**:
1. ‚úÖ **Imm√©diat**: `sample_size: -1`
2. ‚úÖ **Court terme**: Grid Search
3. ‚úÖ **Moyen terme**: Random Forest/XGBoost

**Temps estim√©**: 1-2 jours pour passer √† 60-65%

### 2. Surapprentissage DecisionTree (56%)

**Causes**:
- ‚ùå `max_depth: null` (pas de limite)
- ‚ùå `min_samples_split: 2` (trop faible)
- ‚ùå Arbre trop profond (92 niveaux)

**Solutions**:
1. ‚úÖ **Imm√©diat**: Limiter profondeur (15-20)
2. ‚úÖ **Court terme**: Augmenter min_samples (20-30)
3. ‚úÖ **Recommand√©**: Passer √† Random Forest

**Temps estim√©**: 10 minutes pour fixer

### 3. Features Images Basiques

**Actuel**: Histogrammes RGB (192 features)

**Am√©liorations possibles**:
- HOG features
- SIFT/ORB keypoints
- CNN embeddings (ResNet, VGG)
- Transfer Learning (d√©j√† impl√©ment√© dans TLModel!)

**Impact**: +10-20% accuracy

---

## üéØ Objectifs R√©alistes

| Phase | Actions | Accuracy attendue | Temps |
|-------|---------|-------------------|-------|
| **Actuel** | Test 500 √©chantillons | 37-42% | ‚úÖ Fait |
| **Phase 1** | Dataset complet | 50-55% | 30 min |
| **Phase 2** | Fix overfitting DT | 50-55% | 10 min |
| **Phase 3** | Optimiser TF-IDF | 55-60% | 1h |
| **Phase 4** | Grid Search | 60-65% | 2-4h |
| **Phase 5** | Random Forest | 65-70% | 1h |
| **Phase 6** | XGBoost | 70-75% | 2h |
| **Phase 7** | Transfer Learning | 75-85% | 4-8h |

**Objectif minimum pour production**: **60%** (Phase 4)
**Objectif recommand√©**: **70%** (Phase 6)

---

## üìÅ Structure des Fichiers

```
src/
‚îú‚îÄ‚îÄ preprocessing/
‚îÇ   ‚îú‚îÄ‚îÄ SGDCModel/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ preprocessing.yaml  # Configuration
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ __main__.py         # Lancer: python -m src.preprocessing.SGDCModel
‚îÇ   ‚îî‚îÄ‚îÄ DecisionTreeModel/
‚îÇ       ‚îî‚îÄ‚îÄ ...
‚îî‚îÄ‚îÄ models/
    ‚îú‚îÄ‚îÄ SGDCModel/
    ‚îÇ   ‚îú‚îÄ‚îÄ model_config.yaml   # Configuration
    ‚îÇ   ‚îî‚îÄ‚îÄ __main__.py         # Lancer: python -m src.models.SGDCModel --train
    ‚îî‚îÄ‚îÄ DecisionTreeModel/
        ‚îî‚îÄ‚îÄ ...

models/  # R√©sultats g√©n√©r√©s
‚îú‚îÄ‚îÄ SGDCModel/
‚îÇ   ‚îú‚îÄ‚îÄ artefacts/       # Mod√®le + encodeurs
‚îÇ   ‚îú‚îÄ‚îÄ metrics/         # M√©triques JSON + confusion matrix
‚îÇ   ‚îî‚îÄ‚îÄ visualization/   # Feature importance
‚îî‚îÄ‚îÄ DecisionTreeModel/
    ‚îî‚îÄ‚îÄ ...
```

---

## üöÄ Commandes Rapides

```bash
# SGDC - Preprocessing
python -m src.preprocessing.SGDCModel

# SGDC - Training
python -m src.models.SGDCModel --train

# SGDC - Pr√©diction
python -m src.models.SGDCModel --predict

# DecisionTree - Preprocessing
python -m src.preprocessing.DecisionTreeModel

# DecisionTree - Training
python -m src.models.DecisionTreeModel --train

# DecisionTree - Pr√©diction
python -m src.models.DecisionTreeModel --predict
```

---

## ‚úÖ Checklist Avant Production

- [ ] Dataset complet utilis√© (`sample_size: -1`)
- [ ] Accuracy > 60%
- [ ] Overfitting < 15%
- [ ] F1-weighted > 60%
- [ ] Toutes classes F1 > 30%
- [ ] Grid Search ex√©cut√©
- [ ] Confusion matrix analys√©e
- [ ] Feature importance coh√©rente
- [ ] Tests de pr√©diction valid√©s

---

**Derni√®re mise √† jour**: 2026-01-02
**Status**: ‚ö†Ô∏è Mod√®les fonctionnels mais performances √† am√©liorer
