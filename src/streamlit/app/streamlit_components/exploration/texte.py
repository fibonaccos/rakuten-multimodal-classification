import streamlit as st
import pandas as pd


def render():
    """Affiche l'exploration des donn√©es TEXTE (1m40 de pr√©sentation)"""
    
    st.markdown("## üìù Exploration des Donn√©es Textuelles (X)")
    
    # Structure du dataset
    st.markdown("### üìä Structure du Dataset")
    
    col1, col2 = st.columns(2)
    
    with col1:
        st.info("""
        **X_train.csv : 84 916 lignes √ó 5 colonnes**
        
        - `Unnamed: 0` : Indices (inutile)
        - `designation` : Titres des produits
        - `description` : Descriptions d√©taill√©es
        - `productid` : ID unique produit
        - `imageid` : ID unique image
        """)
        
        st.success("""
        ‚úÖ **4 colonnes utiles** : designation, description, productid, imageid
        
        ‚úÖ **Unicit√©** : Valeurs uniques dans productid et imageid
        ‚Üí Chaque produit est unique dans le dataset
        """)
    
    with col2:
        st.metric("Produits", "84 916", delta="Total dataset")
        st.metric("Variables Texte", "2", delta="designation + description")
        st.metric("IDs Uniques", "100%", delta="productid & imageid")
    
    st.markdown("---")
    
    # Cat√©gories
    st.markdown("### üè∑Ô∏è Analyse des Cat√©gories (Y)")
    
    st.write("""
    **Y_train.csv** : 84 916 lignes √ó 2 colonnes (Unnamed: 0 + prdtypecode)
    
    La colonne `prdtypecode` contient **27 cat√©gories distinctes** √† pr√©dire.
    """)
    
    # Distribution des cat√©gories
    st.markdown("#### Distribution des Cat√©gories")
    
    col1, col2, col3 = st.columns(3)
    with col1:
        st.metric("Cat√©gories", "27", delta="Classes √† pr√©dire")
    with col2:
        st.metric("Classe Max", "2583", delta="12.02% du dataset")
    with col3:
        st.metric("D√©s√©quilibre", "~1:50", delta="Entre min et max")
    
    st.error("""
    **‚ö†Ô∏è D√©s√©quilibre Inter-Classes Majeur**
    
    - **Cat√©gorie 2583** (sur-repr√©sent√©e) : 10 209 produits (12.02%)
    - **Cat√©gories sous-repr√©sent√©es** : 2905, 60, 2220, 1301, 1940, 1180
    
    ‚Üí N√©cessite r√©-√©chantillonnage : sous-√©chantillonnage de 2583 + sur-√©chantillonnage minoritaires
    """)
    
    # Champs lexicaux
    st.markdown("#### Champs Lexicaux et Corr√©lations")
    
    st.write("""
    **Analyse par nuages de mots** (wordcloud) : Construction sur designation + description concat√©n√©es 
    pour r√©v√©ler les univers lexicaux distincts de chaque cat√©gorie.
    
    **Matrice de corr√©lation lexicale** (heatmap) : Calcul des corr√©lations entre cat√©gories pour 
    identifier les champs lexicaux rapproch√©s.
    """)
    
    col1, col2 = st.columns(2)
    
    with col1:
        st.success("""
        **‚úÖ Cat√©gories bien distinctes**
        
        Champs lexicaux clairement identifiables :
        - **2583** : "piscine", "gonflable", "eau"
        - **1280** : "ps4", "jeu", "console"
        - **1920** : "housse", "coton", "lit"
        - **2585** : "outil", "vis", "√©lectrique"
        """)
    
    with col2:
        st.warning("""
        **‚ö†Ô∏è Cat√©gories √† champs rapproch√©s**
        
        Corr√©lation lexicale √©lev√©e (heatmap) :
        - **(1280, 1281)** : Jeux g√©n√©raux vs PC
        - **(50, 2462)** : Accessoires gaming
        - **(1280, 1302)** : Jeux vs Accessoires
        - **(1560, 2582)** : Mobilier int√©rieur/ext√©rieur
        
        ‚Üí Cat√©gorie **1280** particuli√®rement corr√©l√©e
        """)
    
    st.info("""
    üí° **Outils utilis√©s** : Nuages de mots (wordcloud) + Matrice de corr√©lation (heatmap) pour 
    identifier visuellement les similitudes et diff√©rences lexicales entre cat√©gories.
    """)
    
    # === VISUALISATIONS INTERACTIVES ===
    with st.expander("üìä Voir les visualisations (Distribution + Nuages de mots + Langues)"):
        try:
            # Import des fonctions du coll√®gue
            import matplotlib.pyplot as plt
            import seaborn as sns
            from wordcloud import WordCloud
            import re
            from bs4 import BeautifulSoup
            import nltk
            from nltk.corpus import stopwords
            
            # Tentative de chargement des donn√©es
            try:
                # Chemins possibles
                data_paths = [
                    "C:/Users/Peeta/Desktop/Projet/rakuten-multimodal-classification/data/raw/X_train_update.csv",
                    "data/raw/X_train_update.csv",
                    "../../../../../data/raw/X_train_update.csv"
                ]
                
                y_paths = [
                    "C:/Users/Peeta/Desktop/Projet/rakuten-multimodal-classification/data/raw/Y_train_CVw08PX.csv",
                    "data/raw/Y_train_CVw08PX.csv",
                    "../../../../../data/raw/Y_train_CVw08PX.csv"
                ]
                
                df_x = None
                df_y = None
                
                for x_path, y_path in zip(data_paths, y_paths):
                    try:
                        df_x = pd.read_csv(x_path)
                        df_y = pd.read_csv(y_path)
                        break
                    except:
                        continue
                
                if df_x is None:
                    st.warning("‚ö†Ô∏è Donn√©es brutes non disponibles. Visualisations d√©sactiv√©es.")
                else:
                    df = df_x.copy()
                    df['prdtypecode'] = df_y['prdtypecode']
                    
                    # 1. Distribution des classes
                    st.markdown("##### üìä Distribution des Cat√©gories")
                    
                    type_counts = df['prdtypecode'].value_counts()
                    fig1, ax1 = plt.subplots(figsize=(12, 6))
                    type_counts.plot(kind='bar', color=sns.color_palette("viridis", len(type_counts)), ax=ax1)
                    ax1.set_title('Distribution des Types de Produits', fontsize=16, fontweight='bold')
                    ax1.set_xlabel('Type de Produit', fontsize=14)
                    ax1.set_ylabel('Nombre d\'Occurrences', fontsize=14)
                    plt.xticks(rotation=45, fontsize=12)
                    
                    for index, value in enumerate(type_counts):
                        ax1.text(index, value, str(value), ha='center', va='bottom', fontsize=9)
                    
                    plt.tight_layout()
                    st.pyplot(fig1)
                    plt.close()
                    
                    # 2. Nuages de mots
                    st.markdown("##### ‚òÅÔ∏è Nuages de Mots par Cat√©gorie")
                    
                    # Fonction de nettoyage
                    def get_stopwords():
                        try:
                            return set(stopwords.words('french'))
                        except:
                            return set()
                    
                    FINAL_STOPWORDS = get_stopwords()
                    
                    def throw_html_elem(text):
                        if not isinstance(text, str): return ""
                        try:
                            return BeautifulSoup(text, "html.parser").get_text(separator=" ")
                        except:
                            return text
                    
                    def basic_clean(text):
                        if not isinstance(text, str): return ""
                        text = text.lower()
                        text = re.sub(r"[^a-z√†√¢√ß√©√®√™√´√Æ√Ø√¥√ª√π√º√ø√±√¶≈ì\s]", " ", text)
                        text = re.sub(r"\s+", " ", text)
                        words = text.split()
                        words = [w for w in words if w not in FINAL_STOPWORDS and len(w) > 2]
                        return " ".join(words)
                    
                    # Pr√©parer les donn√©es
                    df['lexical_field'] = df['designation'].fillna('') + ' ' + df['description'].fillna('')
                    df['lexical_field'] = df['lexical_field'].apply(lambda s: basic_clean(throw_html_elem(s)))
                    text_by_class = df.groupby('prdtypecode')['lexical_field'].apply(lambda s: ' '.join(s))
                    
                    available_classes = sorted(text_by_class.index.unique())
                    selected_class = st.selectbox("Choisir une classe :", available_classes, index=0)
                    
                    if selected_class:
                        text_content = text_by_class[selected_class]
                        
                        if text_content and len(text_content.strip()) > 0:
                            wc = WordCloud(width=800, height=400, background_color='white', colormap='cividis').generate(text_content)
                            
                            fig_wc, ax_wc = plt.subplots(figsize=(10, 6))
                            ax_wc.imshow(wc, interpolation='bilinear')
                            ax_wc.set_title(f'Nuage de mots - Classe : {selected_class}', fontsize=16)
                            ax_wc.axis('off')
                            
                            st.pyplot(fig_wc)
                            plt.close()
                        else:
                            st.warning(f"Pas assez de mots cl√©s pour la classe {selected_class}")
                    
                    # 3. Distribution des langues
                    st.markdown("##### üåç Distribution des Langues")
                    
                    donnees_langues = {
                        "Fran√ßais": 27000,
                        "Anglais": 7600,
                        "Italien": 5000,
                        "Unknown": 3000,
                        "Roumain": 2500,
                        "Espagnol": 1200
                    }
                    
                    st.bar_chart(donnees_langues)
                    
                    st.caption("""
                    üìå **Note** : Distribution approximative bas√©e sur d√©tection automatique avec `langdetect`.
                    Le fran√ßais domine (~70%), mais pr√©sence significative de langues √©trang√®res (~15-20%).
                    """)
                    
            except Exception as e:
                st.error(f"Erreur lors du chargement des visualisations : {e}")
                
        except ImportError as e:
            st.warning(f"‚ö†Ô∏è Biblioth√®ques manquantes pour les visualisations : {e}")
    
    st.markdown("---")
    
    # Donn√©es textuelles
    st.markdown("### üìù Variables Textuelles : designation & description")
    
    # Valeurs manquantes
    st.markdown("#### üö® Valeurs Manquantes")
    
    col1, col2 = st.columns([2, 1])
    
    with col1:
        st.error("""
        **35.09% de valeurs manquantes** dans `description`
        
        **Cat√©gories les plus touch√©es** (taux > 60%) :
        - **2403** : 88.5%
        - **2462** : 81.2%
        - **2280** : 77.3%
        - **1160** : 69.8%
        - **10** : 65.4%
        - **1180** : 63.7%
        - **40** : 61.5%
        - **1140** : 60.1%
        
        **Explications** :
        - Description = champ non obligatoire sur Rakuten
        - Vendeurs int√®grent parfois description dans le titre
        - Qualit√© variable selon cat√©gories
        """)
    
    with col2:
        st.success("""
        **‚úÖ Cat√©gories propres**
        
        Taux < 5% de manquants :
        - **2905** : 0% (parfait!)
        - **1920** : faible
        - **1560** : faible
        - **2582** : faible
        
        Donn√©es de qualit√© optimale
        """)
    
    # Analyse textuelle
    st.markdown("#### üîç Analyse Textuelle - Qualit√© des Donn√©es")
    
    st.write("""
    **Probl√®mes identifi√©s dans les variables textuelles :**
    """)
    
    quality_data = {
        "Probl√®me": [
            "Code HTML r√©siduel",
            "Caract√®res sp√©ciaux",
            "URLs et emails",
            "Langues √©trang√®res"
        ],
        "Variable": [
            "description (important)",
            "designation (26) / description (47)",
            "Pr√©sents sporadiquement",
            "~15% non-fran√ßais"
        ],
        "Impact": [
            "Pollue le texte et fausse les longueurs",
            "Incompatibilit√©s avec mod√®les",
            "Information non pertinente",
            "N√©cessite encodage multilingue"
        ]
    }
    st.dataframe(pd.DataFrame(quality_data), use_container_width=True, hide_index=True)
    
    st.info("""
    üí° **Constat** : Nettoyage pouss√© n√©cessaire, notamment pour vectorisation TF-IDF 
    (tokenisation bas√©e sur mots). Les transformers multilingues seraient avantageux pour 
    g√©rer les langues √©trang√®res.
    """)
    
    # Distribution des langues
    st.markdown("#### üåç Langues D√©tect√©es")
    
    col1, col2, col3 = st.columns(3)
    with col1:
        st.metric("Fran√ßais", "~85%", delta="Majoritaire")
    with col2:
        st.metric("Anglais", "~8%", delta="Produits internationaux")
    with col3:
        st.metric("Autres", "~7%", delta="Espagnol, Italien, etc.")
    
    # Distribution des longueurs
    st.markdown("#### üìè Distribution des Longueurs de Texte")
    
    st.write("""
    **Analyse variable `designation` :**
    """)
    
    col1, col2 = st.columns(2)
    
    with col1:
        st.markdown("**Avec description pr√©sente**")
        length_with = {
            "M√©trique": ["Moyenne", "√âcart-type", "Observation"],
            "Valeur": ["73.67 car.", "29.81", "Homog√®ne et d√©taill√©"]
        }
        st.table(pd.DataFrame(length_with))
        
        st.success("""
        ‚úÖ Titres plus longs et homog√®nes quand description pr√©sente
        ‚Üí Effort du vendeur, informations pr√©cises
        """)
    
    with col2:
        st.markdown("**Sans description**")
        length_without = {
            "M√©trique": ["Moyenne", "√âcart-type", "Observation"],
            "Valeur": ["63.67 car.", "46.36", "Plus court et h√©t√©rog√®ne"]
        }
        st.table(pd.DataFrame(length_without))
        
        st.warning("""
        ‚ö†Ô∏è **Pic caract√©ristique √† 240-250 car.**
        ‚Üí Limite syst√®me : description int√©gr√©e dans titre
        ‚Üí Vendeurs bloqu√©s par taille maximale
        """)
    
    st.info("""
    üí° **D√©couverte importante** : 
    - Longueur maximale de `designation` : **250 caract√®res**
    - Pic √† 240-250 car. quand pas de description ‚Üí Vendeurs utilisent le titre comme description
    - Strat√©gie possible : Si longueur(titre) > 150 car. ET pas de description ‚Üí Consid√©rer titre comme description
    """)
    
    st.markdown("---")
    
    # Synth√®se
    st.markdown("### üéØ Synth√®se de l'Exploration")
    
    col1, col2 = st.columns(2)
    
    with col1:
        st.success("""
        **‚úÖ Points Forts**
        
        1. Volume suffisant : 84 916 produits
        2. Unicit√© garantie (productid/imageid)
        3. Images uniformes (500√ó500 RGB)
        4. Champs lexicaux distincts majoritairement
        5. 27 cat√©gories = nombre mod√©r√©
        """)
    
    with col2:
        st.error("""
        **‚ö†Ô∏è D√©fis Identifi√©s**
        
        1. **35% descriptions manquantes**
        2. **D√©s√©quilibre 1:50** (cat√©gorie 2583)
        3. Code HTML, caract√®res sp√©ciaux
        4. 15% textes en langues √©trang√®res
        5. Cat√©gories lexicalement proches
        """)
    
    st.write("""
    **‚Üí Ces observations guideront les choix de preprocessing et mod√©lisation.**
    """)
