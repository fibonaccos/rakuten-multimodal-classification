import streamlit as st
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path
import sys

# Ajouter la racine du projet au PYTHONPATH
PROJECT_ROOT = Path(__file__).resolve().parents[5]
if str(PROJECT_ROOT) not in sys.path:
    sys.path.insert(0, str(PROJECT_ROOT))

# Chemins
ASSETS_DIR = Path(__file__).resolve().parent.parent.parent / "assets"


@st.cache_data
def load_data():
    """Charge les donn√©es Y_train"""
    try:
        data_path = PROJECT_ROOT / "data" / "Y_train_CVw08PX.csv"
        if data_path.exists():
            return pd.read_csv(data_path)
    except Exception:
        pass
    return None
    try:
        import nltk
        try:
            nltk.data.find('corpora/stopwords')
        except LookupError:
            nltk.download('stopwords', quiet=True)
    except ImportError:
        pass


def get_all_stopwords():
    """R√©cup√®re tous les stopwords fran√ßais √©tendus"""
    try:
        from nltk.corpus import stopwords
        stopwds = set(stopwords.words('french'))
    except:
        stopwds = set()
    
    added_stopwds = {
        'je', 'tu', 'il', 'elle', 'nous', 'vous', 'ils', 'elles', 'me', 'te', 'se', 'moi', 'toi', 'soi', 'leur', 'lui',
        'en', 'y', 'ce', 'cela', '√ßa', 'ceci', 'celui', 'celle', 'ceux', 'celles','mon', 'ton', 'son', 'notre', 'votre',
        'leur', 'mes', 'tes', 'ses', 'nos', 'vos', 'leurs', 'le', 'la', 'les', 'un', 'une', 'des', 'du', 'de', 'au', 'aux',
        'ce', 'ces', 'cet', 'cette', '√™tre', 'avoir', 'faire', 'aller', 'venir', 'pouvoir', 'devoir', 'savoir', 'dire',
        'voir', 'mettre', 'prendre', 'donner', 'vouloir', 'falloir', 'et', 'ou', 'mais', 'donc', 'or', 'ni', 'car', 'par',
        'pour', 'dans', 'sur', 'sous', 'avec', 'sans', 'entre', 'chez', 'vers', 'selon', 'depuis', 'pendant', 'autour',
        'apr√®s', 'avant', 'si', 'quand', 'comme', 'bien', 'tr√®s', 'trop', 'peu', 'aussi', 'encore', 'd√©j√†', 'toujours',
        'jamais', 'parfois', 'souvent', 'moins', 'plus', 'autant', 'alors', 'ensuite', '√©galement', 'tout', 'tous',
        'toutes', 'chaque', 'aucun', 'certaines', 'certains', 'plusieurs', 'autre', 'autres', 'm√™me', 'tel', 'tels',
        'tellement', 'chose', 'truc', 'cas', 'fa√ßon', 'mani√®re', 'genre', 'type'
    }
    stopwds.update(added_stopwds)
    return stopwds


def throw_html_elem(text: str) -> str:
    """Supprime les √©l√©ments HTML"""
    if not isinstance(text, str): 
        return ""
    try:
        return BeautifulSoup(text, "html.parser").get_text(separator=" ")
    except:
        return text


def basic_clean(text: str, stopwords_set) -> str:
    """Nettoie le texte pour wordcloud"""
    if not isinstance(text, str): 
        return ""
    text = text.lower()
    text = re.sub(r"[^a-z√†√¢√ß√©√®√™√´√Æ√Ø√¥√ª√π√º√ø√±√¶≈ì\s]", " ", text) 
    text = re.sub(r"\s+", " ", text)  
    words = text.split()
    words = [w for w in words if w not in stopwords_set and len(w) > 2]
    return " ".join(words)


@st.cache_data
def process_text_for_wordclouds(df):
    """Pr√©pare les textes par cat√©gorie pour wordclouds"""
    stopwords_set = get_all_stopwords()
    data = df.copy()
    
    data['lexical_field'] = data['designation'].fillna('') + ' ' + data['description'].fillna('')
    data['lexical_field'] = data['lexical_field'].apply(lambda s: basic_clean(throw_html_elem(s), stopwords_set))
    
    grouped_text = data.groupby('prdtypecode')['lexical_field'].apply(lambda s: ' '.join(s))
    
    return grouped_text


def generate_interactive_wordclouds():
    """G√©n√®re les nuages de mots interactifs avec boutons"""
    try:
        from wordcloud import WordCloud
    except ImportError:
        st.warning("‚ö†Ô∏è Module wordcloud non disponible")
        return
    
    download_nltk_resources()
    
    # Charger les donn√©es
    df = load_full_data()
    
    if df is None:
        st.info("üìÅ Donn√©es brutes non disponibles localement - nuages de mots d√©sactiv√©s")
        return
    
    # Pr√©parer les textes
    with st.spinner("Traitement des textes..."):
        text_by_class = process_text_for_wordclouds(df)
    
    available_classes = sorted(text_by_class.index.unique())
    
    # Mapping des cat√©gories
    category_names = {
        10: "Livres/M√©dias", 40: "Jeux vid√©o/Consoles", 50: "Accessoires gaming",
        60: "Consoles", 1140: "Figurines", 1160: "Cartes collectibles",
        1180: "Figurines", 1280: "Jeux (g√©n√©ral)", 1281: "Jeux PC",
        1300: "D√©co int√©rieure", 1301: "D√©co ext√©rieure", 1302: "Accessoires consoles",
        1320: "Mobilier int√©rieur", 1560: "Mobilier ext√©rieur", 1920: "Linge de maison",
        1940: "Literie/Ameublement", 2060: "D√©co murale", 2220: "√âquipement animaux",
        2280: "Magazines", 2403: "Livres (autre type)", 2462: "Jeux/Consoles retro",
        2522: "Papeterie", 2582: "Mobilier ext√©rieur", 2583: "Piscines",
        2585: "Bricolage", 2705: "Livres", 2905: "Jeux de soci√©t√©"
    }
    
    st.write("**S√©lectionnez une cat√©gorie pour visualiser son nuage de mots :**")
    
    # Cr√©er des colonnes pour les boutons (5 boutons par ligne)
    cols_per_row = 5
    rows = [available_classes[i:i+cols_per_row] for i in range(0, len(available_classes), cols_per_row)]
    
    # Session state pour stocker la s√©lection
    if 'selected_wordcloud_class' not in st.session_state:
        st.session_state.selected_wordcloud_class = available_classes[0]
    
    # Afficher les boutons
    for row in rows:
        cols = st.columns(cols_per_row)
        for idx, cat_code in enumerate(row):
            cat_name = category_names.get(cat_code, f"Cat {cat_code}")
            with cols[idx]:
                if st.button(f"{cat_code}\n{cat_name}", key=f"wc_{cat_code}", use_container_width=True):
                    st.session_state.selected_wordcloud_class = cat_code
    
    # Afficher le wordcloud pr√©-g√©n√©r√© pour la cat√©gorie s√©lectionn√©e
    selected_class = st.session_state.selected_wordcloud_class
    st.markdown(f"### Cat√©gorie : **{selected_class}** - *{category_names.get(selected_class, 'N/A')}*")
    
    # Charger l'image pr√©-g√©n√©r√©e
    wordcloud_path = ASSETS_DIR / "wordclouds" / f"wordcloud_{selected_class}.png"
    
    if wordcloud_path.exists():
        st.image(str(wordcloud_path), use_container_width=True)
    else:
        st.warning(f"Nuage de mots non disponible pour la cat√©gorie {selected_class}")


def render():
    """Affiche l'exploration des donn√©es TEXTE (1m40 de pr√©sentation)"""
    
    st.markdown("## üìù Exploration des Donn√©es Textuelles")
    
    # Structure du dataset
    st.markdown("### üìä Structure du Dataset")
    
    col1, col2 = st.columns(2)
    
    with col1:
        st.info("""
        **X_train.csv : 84 916 lignes √ó 5 colonnes**
        
        - `designation` : Titres des produits
        - `description` : Descriptions d√©taill√©es
        - `productid` : ID unique produit
        - `imageid` : ID unique image
        """)
        
        st.success("""
        ‚úÖ **4 colonnes utiles** exploitables
        
        ‚úÖ **Unicit√©** : Chaque produit unique li√© √† une image unique
        ‚Üí Chaque produit est unique dans le dataset
        """)
    
    with col2:
        st.metric("Produits", "84 916", delta="Total dataset")
        st.metric("Variables Texte", "2", delta="designation + description")
        st.metric("IDs Uniques", "100%", delta="productid & imageid")
    
    st.markdown("---")
    
    # Cat√©gories
    st.markdown("### üè∑Ô∏è Analyse des Cat√©gories (Y)")
    
    st.write("""
    **Y_train.csv** : 84 916 lignes √ó 2 colonnes (Unnamed: 0 + prdtypecode)
    
    La colonne `prdtypecode` contient **27 cat√©gories distinctes** √† pr√©dire.
    """)
    
    # Charger les donn√©es
    y_data = load_data()
    
    # Distribution des cat√©gories
    st.markdown("#### Distribution des Cat√©gories")
    
    col1, col2, col3 = st.columns(3)
    with col1:
        st.metric("Cat√©gories", "27", delta="Classes √† pr√©dire")
    with col2:
        st.metric("Classe Max", "2583", delta="12.02% du dataset")
    with col3:
        st.metric("D√©s√©quilibre", "~1:50", delta="Entre min et max")
    
    st.error("""
    **‚ö†Ô∏è D√©s√©quilibre Inter-Classes Majeur**
    
    - **Cat√©gorie 2583** (sur-repr√©sent√©e) : 10 209 produits (12.02%)
    - **Cat√©gories sous-repr√©sent√©es** : 2905, 60, 2220, 1301, 1940, 1180
    
    ‚Üí N√©cessite r√©-√©chantillonnage : sous-√©chantillonnage de 2583 + sur-√©chantillonnage minoritaires
    """)
    
    # Champs lexicaux
    st.markdown("#### Champs Lexicaux et Corr√©lations")
    
    st.write("""
    **Analyse par nuages de mots** (wordcloud) : Construction sur designation + description concat√©n√©es 
    pour r√©v√©ler les univers lexicaux distincts de chaque cat√©gorie.
    
    **Matrice de corr√©lation lexicale** (heatmap) : Calcul des corr√©lations entre cat√©gories pour 
    identifier les champs lexicaux rapproch√©s.
    """)
    
    col1, col2 = st.columns(2)
    
    with col1:
        st.success("""
        **‚úÖ Cat√©gories bien distinctes**
        
        Champs lexicaux clairement identifiables :
        - **2583** : "piscine", "gonflable", "eau"
        - **1280** : "ps4", "jeu", "console"
        - **1920** : "housse", "coton", "lit"
        - **2585** : "outil", "vis", "√©lectrique"
        """)
    
    with col2:
        st.warning("""
        **‚ö†Ô∏è Cat√©gories √† champs rapproch√©s**
        
        Corr√©lation lexicale √©lev√©e (heatmap) :
        - **(1280, 1281)** : Jeux g√©n√©raux vs PC
        - **(50, 2462)** : Accessoires gaming
        - **(1280, 1302)** : Jeux vs Accessoires
        - **(1560, 2582)** : Mobilier int√©rieur/ext√©rieur
        
        ‚Üí Cat√©gorie **1280** particuli√®rement corr√©l√©e
        """)
    
    st.info("""
    üí° **Outils utilis√©s** : Nuages de mots (wordcloud) + Matrice de corr√©lation (heatmap) pour 
    identifier visuellement les similitudes et diff√©rences lexicales entre cat√©gories.
    """)
    
    # === VISUALISATIONS ===
    with st.expander("üìä Voir les visualisations (Distribution + Nuages de mots)"):
        try:
            # Chemins vers les images statiques
            assets_path = Path(__file__).parent.parent.parent / "assets"
            
            # 1. Distribution des classes
            st.markdown("##### üìä Distribution des Cat√©gories")
            class_dist_path = assets_path / "class_distribution.png"
            if class_dist_path.exists():
                st.image(str(class_dist_path), use_container_width=True)
            else:
                st.info("Graphique non disponible")
            
            st.markdown("---")
            
            # 2. Nuages de mots INTERACTIFS
            st.markdown("##### ‚òÅÔ∏è Nuages de Mots par Cat√©gorie (Interactif)")
            generate_interactive_wordclouds()
                    
        except Exception as e:
            st.error(f"Erreur lors du chargement des visualisations : {e}")
    
    st.markdown("---")
    
    # Donn√©es textuelles
    st.markdown("### üìù Variables Textuelles : designation & description")
    
    # Valeurs manquantes
    st.markdown("#### üö® Valeurs Manquantes")
    
    col1, col2 = st.columns([2, 1])
    
    with col1:
        st.error("""
        **35.09% de valeurs manquantes** dans `description`
        
        **Cat√©gories les plus touch√©es** (taux > 60%) :
        - **2403** : 88.5%
        - **2462** : 81.2%
        - **2280** : 77.3%
        - **1160** : 69.8%
        - **10** : 65.4%
        - **1180** : 63.7%
        - **40** : 61.5%
        - **1140** : 60.1%
        
        **Explications** :
        - Description = champ non obligatoire sur Rakuten
        - Vendeurs int√®grent parfois description dans le titre
        - Qualit√© variable selon cat√©gories
        """)
    
    with col2:
        st.success("""
        **‚úÖ Cat√©gories propres**
        
        Taux < 5% de manquants :
        - **2905** : 0% (parfait!)
        - **1920** : faible
        - **1560** : faible
        - **2582** : faible
        
        Donn√©es de qualit√© optimale
        """)
    
    # Analyse textuelle
    st.markdown("#### üîç Analyse Textuelle - Qualit√© des Donn√©es")
    
    st.write("""
    **Probl√®mes identifi√©s dans les variables textuelles :**
    """)
    
    quality_data = {
        "Probl√®me": [
            "Code HTML r√©siduel",
            "Caract√®res sp√©ciaux",
            "URLs et emails",
            "Langues √©trang√®res"
        ],
        "Variable": [
            "description (important)",
            "designation (26) / description (47)",
            "Pr√©sents sporadiquement",
            "~15% non-fran√ßais"
        ],
        "Impact": [
            "Pollue le texte et fausse les longueurs",
            "Incompatibilit√©s avec mod√®les",
            "Information non pertinente",
            "N√©cessite encodage multilingue"
        ]
    }
    st.dataframe(pd.DataFrame(quality_data), use_container_width=True, hide_index=True)
    
    st.info("""
    üí° **Constat** : Nettoyage pouss√© n√©cessaire, notamment pour vectorisation TF-IDF 
    (tokenisation bas√©e sur mots). Les transformers multilingues seraient avantageux pour 
    g√©rer les langues √©trang√®res.
    """)
    
    # Distribution des langues
    st.markdown("#### üåç Langues D√©tect√©es")
    
    col1, col2, col3 = st.columns(3)
    with col1:
        st.metric("Fran√ßais", "~85%", delta="Majoritaire")
    with col2:
        st.metric("Anglais", "~8%", delta="Produits internationaux")
    with col3:
        st.metric("Autres", "~7%", delta="Espagnol, Italien, etc.")
    
    # Graphique distribution des langues
    try:
        assets_path = Path(__file__).parent.parent.parent / "assets"
        lang_dist_path = assets_path / "language_distribution.png"
        if lang_dist_path.exists():
            st.image(str(lang_dist_path), use_container_width=True)
    except Exception:
        pass
    
    # Distribution des longueurs
    st.markdown("#### üìè Distribution des Longueurs de Texte")
    
    st.write("""
    **Analyse variable `designation` :**
    """)
    
    col1, col2 = st.columns(2)
    
    with col1:
        st.markdown("**Avec description pr√©sente**")
        length_with = {
            "M√©trique": ["Moyenne", "√âcart-type", "Observation"],
            "Valeur": ["73.67 car.", "29.81", "Homog√®ne et d√©taill√©"]
        }
        st.table(pd.DataFrame(length_with))
        
        st.success("""
        ‚úÖ Titres plus longs et homog√®nes quand description pr√©sente
        ‚Üí Effort du vendeur, informations pr√©cises
        """)
    
    with col2:
        st.markdown("**Sans description**")
        length_without = {
            "M√©trique": ["Moyenne", "√âcart-type", "Observation"],
            "Valeur": ["63.67 car.", "46.36", "Plus court et h√©t√©rog√®ne"]
        }
        st.table(pd.DataFrame(length_without))
        
        st.warning("""
        ‚ö†Ô∏è **Pic caract√©ristique √† 240-250 car.**
        ‚Üí Limite syst√®me : description int√©gr√©e dans titre
        ‚Üí Vendeurs bloqu√©s par taille maximale
        """)
    
    st.info("""
    üí° **D√©couverte importante** : 
    - Longueur maximale de `designation` : **250 caract√®res**
    - Pic √† 240-250 car. quand pas de description ‚Üí Vendeurs utilisent le titre comme description
    - Strat√©gie possible : Si longueur(titre) > 150 car. ET pas de description ‚Üí Consid√©rer titre comme description
    """)
    
    st.markdown("---")
    
    # Synth√®se
    st.markdown("### üéØ Synth√®se de l'Exploration")
    
    col1, col2 = st.columns(2)
    
    with col1:
        st.success("""
        **‚úÖ Points Forts**
        
        1. Volume suffisant : 84 916 produits
        2. Unicit√© garantie (productid/imageid)
        3. Images uniformes (500√ó500 RGB)
        4. Champs lexicaux distincts majoritairement
        5. 27 cat√©gories = nombre mod√©r√©
        """)
    
    with col2:
        st.error("""
        **‚ö†Ô∏è D√©fis Identifi√©s**
        
        1. **35% descriptions manquantes**
        2. **D√©s√©quilibre 1:50** (cat√©gorie 2583)
        3. Code HTML, caract√®res sp√©ciaux
        4. 15% textes en langues √©trang√®res
        5. Cat√©gories lexicalement proches
        """)
    
    st.write("""
    **‚Üí Ces observations guideront les choix de preprocessing et mod√©lisation.**
    """)
