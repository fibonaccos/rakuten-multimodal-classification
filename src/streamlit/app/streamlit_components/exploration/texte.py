import streamlit as st
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path
import sys

# Ajouter la racine du projet au PYTHONPATH
PROJECT_ROOT = Path(__file__).resolve().parents[5]
if str(PROJECT_ROOT) not in sys.path:
    sys.path.insert(0, str(PROJECT_ROOT))


@st.cache_data
def load_data():
    """Charge les donn√©es Y_train"""
    try:
        data_path = PROJECT_ROOT / "data" / "Y_train_CVw08PX.csv"
        if data_path.exists():
            return pd.read_csv(data_path)
    except Exception:
        pass
    return None


def render():
    """Affiche l'exploration des donn√©es TEXTE (1m40 de pr√©sentation)"""
    
    st.markdown("## üìù Exploration des Donn√©es Textuelles")
    
    # Structure du dataset
    st.markdown("### üìä Structure du Dataset")
    
    col1, col2 = st.columns(2)
    
    with col1:
        st.info("""
        **X_train.csv : 84 916 lignes √ó 5 colonnes**
        
        - `designation` : Titres des produits
        - `description` : Descriptions d√©taill√©es
        - `productid` : ID unique produit
        - `imageid` : ID unique image
        """)
        
        st.success("""
        ‚úÖ **4 colonnes utiles** exploitables
        
        ‚úÖ **Unicit√©** : Chaque produit unique li√© √† une image unique
        ‚Üí Chaque produit est unique dans le dataset
        """)
    
    with col2:
        st.metric("Produits", "84 916", delta="Total dataset")
        st.metric("Variables Texte", "2", delta="designation + description")
        st.metric("IDs Uniques", "100%", delta="productid & imageid")
    
    st.markdown("---")
    
    # Cat√©gories
    st.markdown("### üè∑Ô∏è Analyse des Cat√©gories (Y)")
    
    st.write("""
    **Y_train.csv** : 84 916 lignes √ó 2 colonnes (Unnamed: 0 + prdtypecode)
    
    La colonne `prdtypecode` contient **27 cat√©gories distinctes** √† pr√©dire.
    """)
    
    # Charger les donn√©es
    y_data = load_data()
    
    # Distribution des cat√©gories
    st.markdown("#### Distribution des Cat√©gories")
    
    col1, col2, col3 = st.columns(3)
    with col1:
        st.metric("Cat√©gories", "27", delta="Classes √† pr√©dire")
    with col2:
        st.metric("Classe Max", "2583", delta="12.02% du dataset")
    with col3:
        st.metric("D√©s√©quilibre", "~1:50", delta="Entre min et max")
    
    st.error("""
    **‚ö†Ô∏è D√©s√©quilibre Inter-Classes Majeur**
    
    - **Cat√©gorie 2583** (sur-repr√©sent√©e) : 10 209 produits (12.02%)
    - **Cat√©gories sous-repr√©sent√©es** : 2905, 60, 2220, 1301, 1940, 1180
    
    ‚Üí N√©cessite r√©-√©chantillonnage : sous-√©chantillonnage de 2583 + sur-√©chantillonnage minoritaires
    """)
    
    # Champs lexicaux
    st.markdown("#### Champs Lexicaux et Corr√©lations")
    
    st.write("""
    **Analyse par nuages de mots** (wordcloud) : Construction sur designation + description concat√©n√©es 
    pour r√©v√©ler les univers lexicaux distincts de chaque cat√©gorie.
    
    **Matrice de corr√©lation lexicale** (heatmap) : Calcul des corr√©lations entre cat√©gories pour 
    identifier les champs lexicaux rapproch√©s.
    """)
    
    col1, col2 = st.columns(2)
    
    with col1:
        st.success("""
        **‚úÖ Cat√©gories bien distinctes**
        
        Champs lexicaux clairement identifiables :
        - **2583** : "piscine", "gonflable", "eau"
        - **1280** : "ps4", "jeu", "console"
        - **1920** : "housse", "coton", "lit"
        - **2585** : "outil", "vis", "√©lectrique"
        """)
    
    with col2:
        st.warning("""
        **‚ö†Ô∏è Cat√©gories √† champs rapproch√©s**
        
        Corr√©lation lexicale √©lev√©e (heatmap) :
        - **(1280, 1281)** : Jeux g√©n√©raux vs PC
        - **(50, 2462)** : Accessoires gaming
        - **(1280, 1302)** : Jeux vs Accessoires
        - **(1560, 2582)** : Mobilier int√©rieur/ext√©rieur
        
        ‚Üí Cat√©gorie **1280** particuli√®rement corr√©l√©e
        """)
    
    st.info("""
    üí° **Outils utilis√©s** : Nuages de mots (wordcloud) + Matrice de corr√©lation (heatmap) pour 
    identifier visuellement les similitudes et diff√©rences lexicales entre cat√©gories.
    """)
    
    # === VISUALISATIONS STATIQUES ===
    with st.expander("üìä Voir les visualisations (Distribution + Nuages de mots + Langues)"):
        try:
            # Chemins vers les images statiques
            assets_path = Path(__file__).parent.parent.parent / "assets"
            
            # 1. Distribution des classes
            st.markdown("##### üìä Distribution des Cat√©gories")
            class_dist_path = assets_path / "class_distribution.png"
            if class_dist_path.exists():
                st.image(str(class_dist_path), use_container_width=True)
            else:
                st.info("Graphique non disponible")
            
            st.markdown("---")
            
            # 2. Nuages de mots
            st.markdown("##### ‚òÅÔ∏è Nuages de Mots pour Quelques Cat√©gories")
            wordcloud_path = assets_path / "wordclouds.png"
            if wordcloud_path.exists():
                st.image(str(wordcloud_path), use_container_width=True)
            else:
                st.info("Nuages de mots non disponibles")
            
            st.markdown("---")
            
            # 3. Distribution des langues
            st.markdown("##### üåç Distribution des Langues")
            lang_dist_path = assets_path / "language_distribution.png"
            if lang_dist_path.exists():
                st.image(str(lang_dist_path), use_container_width=True)
            else:
                st.info("Graphique non disponible")
                    
        except Exception as e:
            st.error(f"Erreur lors du chargement des visualisations : {e}")
    
    st.markdown("---")
    
    # Donn√©es textuelles
    st.markdown("### üìù Variables Textuelles : designation & description")
    
    # Valeurs manquantes
    st.markdown("#### üö® Valeurs Manquantes")
    
    col1, col2 = st.columns([2, 1])
    
    with col1:
        st.error("""
        **35.09% de valeurs manquantes** dans `description`
        
        **Cat√©gories les plus touch√©es** (taux > 60%) :
        - **2403** : 88.5%
        - **2462** : 81.2%
        - **2280** : 77.3%
        - **1160** : 69.8%
        - **10** : 65.4%
        - **1180** : 63.7%
        - **40** : 61.5%
        - **1140** : 60.1%
        
        **Explications** :
        - Description = champ non obligatoire sur Rakuten
        - Vendeurs int√®grent parfois description dans le titre
        - Qualit√© variable selon cat√©gories
        """)
    
    with col2:
        st.success("""
        **‚úÖ Cat√©gories propres**
        
        Taux < 5% de manquants :
        - **2905** : 0% (parfait!)
        - **1920** : faible
        - **1560** : faible
        - **2582** : faible
        
        Donn√©es de qualit√© optimale
        """)
    
    # Analyse textuelle
    st.markdown("#### üîç Analyse Textuelle - Qualit√© des Donn√©es")
    
    st.write("""
    **Probl√®mes identifi√©s dans les variables textuelles :**
    """)
    
    quality_data = {
        "Probl√®me": [
            "Code HTML r√©siduel",
            "Caract√®res sp√©ciaux",
            "URLs et emails",
            "Langues √©trang√®res"
        ],
        "Variable": [
            "description (important)",
            "designation (26) / description (47)",
            "Pr√©sents sporadiquement",
            "~15% non-fran√ßais"
        ],
        "Impact": [
            "Pollue le texte et fausse les longueurs",
            "Incompatibilit√©s avec mod√®les",
            "Information non pertinente",
            "N√©cessite encodage multilingue"
        ]
    }
    st.dataframe(pd.DataFrame(quality_data), use_container_width=True, hide_index=True)
    
    st.info("""
    üí° **Constat** : Nettoyage pouss√© n√©cessaire, notamment pour vectorisation TF-IDF 
    (tokenisation bas√©e sur mots). Les transformers multilingues seraient avantageux pour 
    g√©rer les langues √©trang√®res.
    """)
    
    # Distribution des langues
    st.markdown("#### üåç Langues D√©tect√©es")
    
    col1, col2, col3 = st.columns(3)
    with col1:
        st.metric("Fran√ßais", "~85%", delta="Majoritaire")
    with col2:
        st.metric("Anglais", "~8%", delta="Produits internationaux")
    with col3:
        st.metric("Autres", "~7%", delta="Espagnol, Italien, etc.")
    
    # Distribution des longueurs
    st.markdown("#### üìè Distribution des Longueurs de Texte")
    
    st.write("""
    **Analyse variable `designation` :**
    """)
    
    col1, col2 = st.columns(2)
    
    with col1:
        st.markdown("**Avec description pr√©sente**")
        length_with = {
            "M√©trique": ["Moyenne", "√âcart-type", "Observation"],
            "Valeur": ["73.67 car.", "29.81", "Homog√®ne et d√©taill√©"]
        }
        st.table(pd.DataFrame(length_with))
        
        st.success("""
        ‚úÖ Titres plus longs et homog√®nes quand description pr√©sente
        ‚Üí Effort du vendeur, informations pr√©cises
        """)
    
    with col2:
        st.markdown("**Sans description**")
        length_without = {
            "M√©trique": ["Moyenne", "√âcart-type", "Observation"],
            "Valeur": ["63.67 car.", "46.36", "Plus court et h√©t√©rog√®ne"]
        }
        st.table(pd.DataFrame(length_without))
        
        st.warning("""
        ‚ö†Ô∏è **Pic caract√©ristique √† 240-250 car.**
        ‚Üí Limite syst√®me : description int√©gr√©e dans titre
        ‚Üí Vendeurs bloqu√©s par taille maximale
        """)
    
    st.info("""
    üí° **D√©couverte importante** : 
    - Longueur maximale de `designation` : **250 caract√®res**
    - Pic √† 240-250 car. quand pas de description ‚Üí Vendeurs utilisent le titre comme description
    - Strat√©gie possible : Si longueur(titre) > 150 car. ET pas de description ‚Üí Consid√©rer titre comme description
    """)
    
    st.markdown("---")
    
    # Synth√®se
    st.markdown("### üéØ Synth√®se de l'Exploration")
    
    col1, col2 = st.columns(2)
    
    with col1:
        st.success("""
        **‚úÖ Points Forts**
        
        1. Volume suffisant : 84 916 produits
        2. Unicit√© garantie (productid/imageid)
        3. Images uniformes (500√ó500 RGB)
        4. Champs lexicaux distincts majoritairement
        5. 27 cat√©gories = nombre mod√©r√©
        """)
    
    with col2:
        st.error("""
        **‚ö†Ô∏è D√©fis Identifi√©s**
        
        1. **35% descriptions manquantes**
        2. **D√©s√©quilibre 1:50** (cat√©gorie 2583)
        3. Code HTML, caract√®res sp√©ciaux
        4. 15% textes en langues √©trang√®res
        5. Cat√©gories lexicalement proches
        """)
    
    st.write("""
    **‚Üí Ces observations guideront les choix de preprocessing et mod√©lisation.**
    """)
