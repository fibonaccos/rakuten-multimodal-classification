import streamlit as st
import pandas as pd
import json
from pathlib import Path

def render():
    """Affiche la section SGDClassifier (1m30 de pr√©sentation)"""
    
    # Introduction
    col1, col2 = st.columns([2, 1])
    
    with col1:
        st.markdown("### üéØ Pr√©sentation des diff√©rentes √©tapes")
        st.write("""
       
        - Nettoyage Syntaxique
        - Gestion des diff√©rents languages
        - Traitement des descriptions vides
        - Vectorisation
                """)
    
    # Hyperparam√®tres optimis√©s
    st.markdown("### üßπ Nettoyage Syntaxique")

    st.write(""" La premi√®re √©tape consiste √† standardiser la forme du texte
             - Encodage & HTML : Suppression des entit√©s HTML (ex: &amp;, <div>) et d√©codage des caract√®res sp√©ciaux via html.unescape.
             - Normalisation Unicode : Transformation des caract√®res accentu√©s en leur √©quivalent ASCII (ex: "√©t√©" -> "ete") via la norme NFKD
             - Casse : Conversion de l'ensemble du texte en minuscules pour √©viter que "Table" et "table" soient consid√©r√©s comme deux mots diff√©rents.
""")
    
    st.markdown("### ‚öôÔ∏è Filtrage des expressions r√©guli√®res (Regex) ")

    st.write(""" Pour r√©duire la dimensionnalit√©, nous appliquons un filtre strict √† savoir la conservation exclusive des lettres (a-z)
""")
    st.code("""
            text = re.sub(r'<[^>]+>', ' ', text)       # HTML
            text = re.sub(r'\b\w*\d\w*\b', ' ', text)  # Mots avec chiffres 
            text = re.sub(r'[^a-z\s]', ' ', text)      # Lettres a-z uniquement
            text = re.sub(r'\s+', ' ', text).strip()   # Espaces        
        """, language="python")
    
    st.markdown("### ‚õî  Suppression des STOPWORDS ")

    st.write(""" Nous utilisons une liste d'exclusion personnalis√©e (bas√©e sur NLTK et enrichie manuellement) pour retirer les mots tr√®s fr√©quents mais peu informatifs
""")
    st.markdown("### ‚úÖÔ∏è Lemmatisation ")

    st.write(""" Plut√¥t que la racinisation (stemming) qui coupe brutalement les mots, nous avons opt√© pour la lemmatisation via la librairie spaCy (mod√®le fr_core_news_sm).
             Le principe est de transformer chaque mot en sa forme canonique. 
""")
    st.markdown("### ‚¨ÜÔ∏è Gestion des Donn√©es Manquantes & Augmentation ")

    st.write(""" - **Valeurs nulles** : Les descriptions manquantes (NaN) ne sont pas supprim√©es mais remplac√©es par des cha√Ænes vides ou fusionn√©es avec la colonne designation (titre du produit) pour maximiser l'information disponible.
             - Traduction : A l'aide de Spacy, on √† d√©cid√© de traduire les 5 langues les plus repr√©sent√©es dans notre dataset
""")
    
    st.markdown("### üìà Vectorisation ")

    st.write(""" Nous avons opt√© pour la m√©thode TF-IDF (Term Frequency - Inverse Document Frequency), qui est une approche statistique robuste privil√©giant les mots "porteurs de sens" sp√©cifiques √† chaque cat√©gorie.
             Nous avons utilis√© TfidfVectorizer de Scikit-Learn avec les hyperparam√®tres suivants, optimis√©s pour maximiser le ratio performance/m√©moire :
             """)
    st.code("""
            vectorizer = TfidfVectorizer(ngram_range=(1,2), #permet d'analyser les mots et les paires de mots 
            min_df=3, # Un mot (ou bigramme) doit appara√Ætre dans au moins 3 produits diff√©rents pour √™tre conserv√©.
            max_features = 10 000 # Nous ne gardons que les 10 000 mots les plus fr√©quents du corpus.)
""", language="python")
    

    